<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<!--
This is an example weekly progress report document that team members can use to report their individual progress 
of their ECE477 senior design projects. Weekly progress reports are expected to follow the general guidelines
presented in the "Progress Report Policy" document, posted on Brightspace.  

Please create 4 copies of this example, renaming each copy to <PurdueID>.html, where <PurdueID> corresponds to
the Purdue ITAP Career Account ID given by Purdue to each individual team member. If you have any questions,
contact course staff.
-->
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />

<!--Reconfigurable base tag; used to modify the site root location for root-relative links-->
<!--<base href="https://engineering.purdue.edu/ece477/StudentWebTemplate/" />-->
    <base href="https://engineering.purdue.edu/477grp5/" /> <!-- Replace the N with your team number-->

<!--Content-->
<title>ECE477 Course Documents</title>
<meta name="keywords" content="" />
<meta name="description" content="" />
<meta name="author" content="George Hadley">
<meta name = "format-detection" content = "telephone=no" />
<meta name="viewport" content="width=device-width,initial-scale=1.0">

<!--CSS-->
<link rel="stylesheet" href="css/default.css" type="text/css" media="all" />
<link rel="stylesheet" href="css/responsive.css">
<link rel="stylesheet" href="css/styles.css">
<link rel="stylesheet" href="css/content.css">
<!--[if IE 6]>
<link href="default_ie6.css" rel="stylesheet" type="text/css" />
<![endif]-->

</head>
<body>
<div id="wrapper_site">
    <div id="wrapper_page">
	<!-- Instantiate global site header.-->
	<div id="header"></div>
		<!-- Instantiate site global navigation bar.-->
		<div id="menu"></div>
	
		<!-- Instantiate a page banner image. Page banner images should be 1100x350px and should be located within the local
			img folder located at this directory level. -->
		<div id="banner">
			<img src="Files/img/BannerImgExample.jpg"></img>
		</div>
	
		<!-- Instantiate "tools" needed for a page. Tools are premade functional blocks that can be used to build a page,
			and include things such as a file lister (for listing out homework assignments or tutorials)
		-->
		
		<div id="content">
            <h2>Progress Report/Engineering Project Journal for Maximilian Drach</h2>
		
			<b>Date Reported:</b> 02/08/2024<br>
			<b>Start Time:</b> 10:30am <br>
			<b>Work Time:</b> 9 hours<br>
			Robotics Kinematic Mappings:
			<ul>
				<li>Collaborated with Yusuf Jarada to develop mappings from motor angles to the x, y, z Cartesian coordinates of the Dodgebot's punching bag.</li>
				<br>
				<label>Robot Angle Kinematics Equations (RAKE)</label>
				<br>
				<img src="Team\progress\img - member3\Robot_Angle_Kinematics_Equ.png" width="50%"></img>
				<br>
				
				<li>Derived inverse mappings to track the Dodgebot's punching bag location with x, y, z Cartesian coordinates and derive the motor position angles, essential for precise robot kinematics across 3D space and tracking the robot with a top-down camera.</li>
				<br>
				<label>Inverse Derivations</label>
				<br>
				<img src="Team\progress\img - member3\Inv_RAKE_derivation.png" width="50%"></img>
				<br>
				<li>This development enables accurate positioning of the robot in response to punches, facilitating testing of communication protocols and validating command execution.</li>
			</ul>

			
			Camera Dodgebot Position Tracking:
			<ul>
				<li>Created an algorithm to track the punching bag utilizing the top-down camera and the assumption of the hemi-spherical nature of the robot’s dodging system. (Video Below & Algorithm Below)</li>
				<br>
				<label>Dodgebot Pixel to Cordinate Mapping (DP2C) Algorithm</label>
				<br>
				<img src="Team\progress\img - member3\Dodgebot_Robot_Angle_Kinematics_Equ.png" width="60%"></img>
				<br>
				<li>The implementation of the new formulas and algorithms allow us to accurately track our robot with a top-down camera down to 1~2 cm accuracy. This is critical if we need an alternative method to track the robot’s position, if we are unable to utilize the motor driver position tracking capabilities.</li>
				<li>By providing precise tracking capabilities, the project gains flexibility in testing and validating robot movements, contributing to overall project robustness.</li>
				<li>Next steps involve integrating the robot tracking into the main code, assessing its impact on processing runtime, and potentially optimizing tracking through threading or other improvements.</li>
				<br>
				<label>Dodgebot Pixel to Cordinate Mapping (DP2C) Algorithm Explaination Video</label>
				<br>
				<video width="99%" controls>
					<source src="Team\progress\img - member3\DP2C_Algorithm_Explination.mp4#t=6,315" type="video/mp4">
				</video> 
				<br>
			</ul>


			<b>Date Reported:</b> 02/06/2024<br>
			<b>Start Time:</b> 9:30am <br>
			<b>Work Time:</b> 5 hours<br>
			
			Bug Fixes:
			<ul>
				<li>Identified and resolved a bug within the Camera Mapping process, updating the formula to account for the similar triangle theory and adjusting for the camera height. (Formula Below)</li>
				<br>
				<label>Updated Pixel to Cordinate Mapping (P2C) Algorithm</label>
				<br>
				<img src="Team\progress\img - member3\Similar_Tri_Camera_Mapping.png" width="95%"></img>
				<br>
				<li>Fixed two additional bugs in the dodging program: addressed a vector value issue in the if statements and implemented inverse proportionality fix to the code.</li>
				<li>These critical fixes are anticipated to enhance our dodging capabilities significantly.</li>
				<li>I want to conduct thorough testing and validation of bug fixes and algorithm enhancements to ensure seamless operation.</li>
				<br>
				<label>Pixel to Cordinate Mapping (DP2C) Algorithm Explaination Video</label>
				<br>
				<video width="99%" controls>
					<source src="Team\progress\img - member3\P2C_Algorithm_Explination.mp4#t=1,197" type="video/mp4">
				</video> 
				<br>
			</ul>
			
			Further Runtime Evaluations:
			<ul>
				<li>Implemented a runtime dictionary to track different parts of the code's runtimes, facilitating performance evaluation and identifying potential bottlenecks.</li>
				<li>Established a dodge review system to record frames associated with dodging, enabling thorough analysis of code behavior and areas for improvement.</li>
				<li>Recognized the significance of runtime evaluations in identifying optimization opportunities and enhancing code efficiency.</li>
			</ul>
			Component Procurement & Selection:
			<ul>
				<li>Collaborated with Jessica Davis from Motion USA to finalize component selection and pricing, securing the Yaskawa SGM7G-30A6A61, SGM7G-20A6A61, SGD7S-330A00A, and critical cables at $8,800 with a lead time of 3~5 days.</li>
				<li>This procurement ensures timely acquisition of essential components, advancing project timelines and readiness.</li>
				<li>Next steps will be to make sure that our components are ordered next week and contact the department about the status of the ordering process.</li>
			</ul>
			

			<b>Date Reported:</b> 02/05/2024<br>
			<b>Start Time:</b> 8:00pm <br>
			<b>Work Time:</b> 2 hours<br>
			<ul>
				<li>I focused on contacting 10 companies to request quotes for Yaskawa components essential for our project. Through email correspondence, I reached out to Nex Instruments, Motion USA, WI Automation, Viyork Tech, Mitkco, Dreisilker, Dykman, Reading CNC, Int Technics, and Motion Industries. The objective was to gather pricing and availability information from various suppliers to aid in our decision-making process. This initiative aimed to ensure we select the most suitable supplier offering competitive pricing and reliable delivery times. The responses received will play a crucial role in assessing factors like cost, lead time, and customer service, which are vital for making informed procurement decisions.</li>
			</ul>
			
			

			<b>Date Reported:</b> 02/02/2024<br>
			<b>Start Time:</b> 1:00pm <br>
			<b>Work Time:</b> 2 hours<br>
			Ordering Motors from Yaskawa:
			<ul>
				<li>After consultation with Dr. Walters, the decision was made to transition our motor selection to Yaskawa motors due to their extensive off-the-shelf component inventory, promising significantly reduced lead times.</li>
				<li>Collaborated with Bob from Finch Automation to facilitate the motor procurement process. Conducted extensive research on available motors and liaised with Bob to ascertain their availability and suitability for our project requirements.</li>
				<li>This shift in motor selection strategy is anticipated to streamline our supply chain logistics and expedite the acquisition of essential components, ultimately enhancing project timelines and overall efficiency.</li>
				<li>I need to finalize the motor selection and proceed with the procurement process through Finch Automation.</li>
				<li>The decision to pivot our motor selection to Yaskawa, coupled with proactive collaboration with Finch Automation, represents a strategic move aimed at expediting component acquisition and enhancing project readiness and efficiency.</li>
				
			</ul>
			

			<b>Date Reported:</b> 02/01/2024<br>
			<b>Start Time:</b> 5:30pm <br>
			<b>Work Time:</b> 4 hours<br>
			
			Optimized K-Means Algorithm:
			<ul>
				<li>Hypothesized that pre-passing center points into the K-Means Algorithm would improve cluster tracking across frames and expedite convergence.</li>
				<li>Implemented enhancements, resulting in a significant reduction in processing time from around 8ms to 2ms per frame, while consistently tracking gloves across frames.</li>
				<br>
				<label>Optimized Kmeans Example Vlog with Ayman Motoda &#x1F60E;</label>
				<br>
				<video width="35%" controls playbackRate="0.25">
					<source src="Team\progress\img - member3\Optimized_Kmean_Ayman.mp4" type="video/mp4">
				</video> 
				<br>
			</ul>
			

			Integrated the Camera De-Distortion Algorithm:
			<ul>
				<li>Utilized the Intrinsic Camera Matrix and Distortion Coefficients derived from 01/30 research to integrate the de-distortion algorithm into the Punch Tracking Algorithm.</li>
				<li>Implemented to mitigate lens distortion effects and improve tracking accuracy.</li>
				<li>Discovered that de-distortion adds approximately 1ms to the Punch Tracking Algorithm's runtime but ensures more accurate tracking without distortion errors.</li>
				<li>This integration enhances the overall accuracy and reliability of the punch tracking system, contributing to the project's success in real-world applications.</li>
				<li>In the video below, the red circle represents the main punch that Dodgebot (white/black circle) has to avoid. Please watch the video at .25 speed to really see the algorithm in motion.</li>
				<br>
				<label>Dodgebot Dodging Algorithm Example *WATCH at .25 Speed!&#x1F600;</label>
				<br>
				<video width="50%" controls playbackRate="0.25">
					<source src="Team\progress\img - member3\Dodging_Algorithm_Example_020224.mp4" type="video/mp4">
				</video> 
				
			</ul>
			
			Insight into Algorithm Efficiency & Operational Effectiveness:
			<ul>
				<li>I learned that our Punch Tracking Algorithm takes around 5~6ms to run and our Punch Dodging Algorithm takes around .5~2ms, this means our end decision dodging time is around 6~8ms.</li>
				<li>This means we process 125 fps ~ 165 fps, giving us around 12~17 frames before a punch is expected to reach a target.</li>
				<li>This is a huge operational advantage and gives us more headroom for delays in communications.</li>
				<li>While ideally, we would want our total algorithm runtime to be around 4ms to run at the native frame rate of the camera of 260fps, this is still a significant improvement.</li>
				<br>
				<label>Algorithm Run Time (sec)</label>
				<br>
				<img src="Team\progress\img - member3\Average_Runtime_020224.png" width="75%"></img>
				<br>
			</ul>


			<b>Date Reported:</b> 02/01/2024<br>
			<b>Start Time:</b> 1:00pm <br>
			<b>Work Time:</b> 3.5 hours<br>
			Tested Kmeans vs Blob Detection:
			<ul>
				<li>Implemented <a href="https://en.wikipedia.org/wiki/Blob_detection" target="_blank"> Determinant of Hessian Blob Detection Algorithm</a> using the SciPy library based on the recommendation of Rohan Sakar.</li>
				<li>Blob Detection ran faster than the K-Means Algorithm, approximately 5ms, but exhibited significantly lower accuracy in detecting gloves and struggled to consistently track punches across frames.</li>
				<li>Next steps involve exploring alternative algorithms or refining parameters to improve Blob Detection accuracy or improve the efficiency of the K-Means Algorithm approach.</li>
			</ul>


			<b>Date Reported:</b> 01/31/2024<br>
			<b>Start Time:</b> 3:00pm <br>
			<b>Work Time:</b> 1 hours<br>
			<ul>
				<li>Collaborated with Ayman Motoda to place an order for motors and motor drivers from Kollmorgen supplier Neff-Automation.</li>
				<li>Discovered that the lead time for the AKM2G-53L-ACDNLD10 motor was 4 weeks, AKD2G-SPC-6V012S-A1DX-0000-A motor driver was 6 weeks, and H2-21-015-B2-00–001000 cable was 6 weeks.</li>
				<li>Explored alternative suppliers and companies for motors with similar specifications and quicker delivery times. Initiated contact with Bob from Finch Automation to explore Yaskawa offerings.</li>
				<li>This proactive approach ensures timely procurement of essential components, contributing to project timeline adherence and overall progress.</li>
            </ul>

			<b>Date Reported:</b> 01/30/2024<br>
			<b>Start Time:</b> 10:00am<br>
			<b>Work Time:</b> 5 hours<br>
			Camera Distortion Research:
			<ul>
                <li>I focused on advancing our project's capabilities through Camera Distortion Research.</li>
                <li>Explored camera lens distortion mapping and implemented <a href="https://ieeexplore.ieee.org/document/888718" target="_blank">Zhang’s calibration method</a> for reverse mapping of the camera’s lens.</li>
                <li>Utilized the OpenCV library to identify edges in a chessboard and derive the Intrinsic Camera Matrix and Distortion Coefficients.</li>
                <li>Achieved undistorted frame algorithm (Picture Below)</li>
                <!-- <img src="Team\progress\img - member3\chessboard_undistorted.jpg" width="75%"></img> -->
				<!-- <img src="Team\progress\img - member3\chessboard_distorted.jpg" width="75%"></img> -->
				<div style="display: flex;">
					<div style="flex: 50%; padding: 5px;">
						<img src="Team\progress\img - member3\chessboard_distorted.jpg" width="100%">
						<p style="text-align: center;">Distorted Image</p>
					</div>
					<div style="flex: 50%; padding: 5px;">
						<img src="Team\progress\img - member3\chessboard_undistorted.jpg" width="100%">
						<p style="text-align: center;">Undistorted Image</p>
					</div>
				</div>
				
                <li>Upgrades our robots’ tracking capabilities to precisely track punches closer to the edges of the camera frame where distortion is most prevalent.</li>
				<li>Acquired valuable insights into camera lens distortion mapping and its implications for accurate punch tracking.</li>
				<li>Next steps involve integrating the undistortion process into our robot's tracking algorithms and rigorously testing its efficacy across diverse scenarios.</li>
				<li>My progress in camera distortion research increases the accuracy of the tracking algorithm thus contributing to an improve punch-dodging algorithm.</li>
				
            </ul>


			<b>Date Reported:</b> 01/25/2024<br>
			<b>Start Time:</b> 10:00am<br>
			<b>Work Time:</b> 9 hours<br>
            Bug Fixes & New Dodging Algorithm:
            <ul>
                <li>Implemented new changes to the computer vision tracking algorithm.</li>
                <li>First, I fixed bugs in the punch location stream buffer. After realizing that we only had around 5 frames per punch to detect and move, I changed the buffer size to 3 frames.</li>
                <li>Next, I analyzed the runtime of my code and realized that the k-means algorithm was running too slow on the CPU, so I rewrote the code to run on the GPU. This improvement reduced my runtime from 45ms to under 15ms per frame. This enhancement allows us to run the camera at 60 fps with theoretically no latency.</li>
                <li>The significant improvement this week was our new dodging algorithm. It is designed around the robot dodging the punch into space and moving in the inverse-opposite proportional direction. (Picture Below)</li>
				<br>
				<label>Punch Dodging Algorithm</label>
				<br>
                <img src="Team\progress\img - member3\Dodge_0126_Algorithm.png" width="75%"></img>
                <li>Finally, I changed all my code to use PyTorch instead of NumPy to allow for further GPU utilization. However, this ended up slowing down the code because arithmetic operations are slower in PyTorch than in NumPy.</li>
            </ul>

			<b>Date Reported:</b> 01/23/2024<br>
			<b>Start Time:</b> 9:30am<br>
			<b>Work Time:</b> 7 hours<br>
            Computer Vision Tracking:
            <ul>
                <li>After applying the color mask filter, I am left with the pixel values that detected by the computer to match the color gloves. I then convert these pixels into their coordinate values in the xy-real-coordinate-space. I then pass the collection of these points into a simple kmeans algorithm which segment these points into two cluster of points.</li>
            </ul>
            Robot Dodging Algorithm Design & Testing:
            <ul>
                <li>I then started to log the center of these clusters from the past 10 frames (i.e., 10 past center points).</li>
                <li>Utilizing this buffer, I find the cluster which has the most change in the distance over the past 10 frames. This cluster is then used as the main punch to track and react too.</li>
                <li>I first then approximate the linear trajectory of punch, and test to see robot location is within (.15m or specified distance) of the punch trajectory line. If the robot is within the punch trajectory, then dodging algorithm is engaged.</li>
                <li>The dodging algorithm checks to see if the punch is mainly coming from the x or y direction. It then places a greater avoidance distance emphasis on the direction that punch is not mainly coming from. So if you are proportionally travelling in the x direction towards the robot, then robot will main move in the y-direction to avoid the punch. If you are coming from an angle where the x and y direction is about equal, then it will mostly travel perpendicular to the incoming punch. In the non-significant direction, it will randomly choose to stay on the same axis (0), move back (-1), more forward (1) equally. This adds more stochasticity to the dodge, without affecting the effectiveness of the dodge.</li>
                <li>Finally, I noticed that the robot would sometime get confused when the punch was pulled back after punching and it would unintentionally move the robot, so I added an extract check to make sure the vector of the punch was moving towards the punch.</li>
            </ul>


			<b>Date Reported:</b> 01/16/2024<br>
			<b>Start Time:</b> 9:30am<br>
			<b>Work Time:</b>7 hours<br>
            Camera Calibration Research:
			<ul>
				<li>Assuming for a non-distortion lens, I derived a formula for mapping a pixel location to its xy-coordinate in the real world. (Formula Below)</li>
				<br>
				<label>Non-Distorted Pixel Mapping Formula</label>
				<br>
                <img src="Team\progress\img - member3\NonDistortion_PixelMap_Formula.png" width="50%"></img>
				<li>Set up a top-down test camera that was 2m above ground.</li>
				<li>Using the environment tracking research from last week, I created a 1x1m box with painters’ tape to test the accuracy of the coordinate mapping. (Picture Below)</li>
				<br>
				<label>Top-Down Camera Setup</label>
				<br>
                <img src="Team\progress\img - member3\Dodgebot_Topdown_Camera_Setup.jpg" width="25%"></img>
				<li>Putting together the color tracking algorithm and camera calibration mapping formula, I created a rough Environment Punch Tracker Algorithm that can accurately track a punch in space. Given the inputted camera observation height, real-world distance from the center pixel to the edge, and user punch height (i.e. the height of a user's hands when in a punching stance), and tracking color HSV values.</li>
				<br>
				<label>Real-XY-Location Color Tracking Video Example</label>
				<br>
				<video width="40%" controls>
                    <source src="Team\progress\img - member3\Punch_Tracking_0116.mp4" type="video/mp4">
                </video> 
				<br>
                <li>Finally, I started researching more advanced camera calibration methods and other M12 lens optics to increase the accuracy of the Camera Calibration Mapping.</li>
				<li>Useful Links:
					<ul>
					  <li><a href="https://www.mathworks.com/help/vision/ug/camera-calibration.html" target="_blank">Camera Calibration - MathWorks</a></li>
					  <li><a href="https://ieeexplore.ieee.org/document/888718" target="_blank">IEEE Explore - Camera Calibration</a></li>
					  <li><a href="https://www.image-engineering.de/library/image-quality/factors/1062-distortion#:~:text=The%20bending%20of%20a%20straight,percentage%20of%20picture%20height%20distortion." target="_blank">Image Engineering - Distortion</a></li>
					  <li><a href="https://docs.opencv.org/4.x/dc/dbb/tutorial_py_calibration.html" target="_blank">OpenCV - Camera Calibration Tutorial</a></li>
					  <li><a href="https://docs.opencv.org/3.4/d4/d94/tutorial_camera_calibration.html" target="_blank">OpenCV - Camera Calibration</a></li>
					  <li><a href="https://docs.opencv.org/4.x/d9/d0c/group__calib3d.html" target="_blank">OpenCV - Camera Calibration (Calib3d Module)</a></li>
					  <li><a href="https://learnopencv.com/camera-calibration-using-opencv/" target="_blank">Learn OpenCV - Camera Calibration</a></li>
					  <li><a href="https://www.geeksforgeeks.org/camera-calibration-with-python-opencv/" target="_blank">GeeksforGeeks - Camera Calibration with Python OpenCV</a></li>
					  <li><a href="https://commonlands.com/products/stereographic-m12-fisheye-cil207?" target="_blank">Stereographic M12 Fisheye Lens</a></li>
					</ul>
				  </li>
			  </ul>
            <b>Date Reported:</b> 01/11/2024<br>
			<b>Start Time:</b> 9:30am<br>
			<b>Work Time:</b>6 hours<br>
			<ul>
				<li>Utilizing the OpenCV Library in Python, I was able to stream the camera frames to a Dell XPS9710 over a USB2 cable.</li>
				<li>Implemented a Color Mask on the video stream utilizing the OpenCV Library in Python.</li>
				<li>I converted the BGR image to HSV color space. The HSV color space allows determination of saturation (Saturation) and brightness (Value), crucial for image sensor calibration. The Hue value controls the color to look for in the image.</li>
				  
				<ul>
					<li><a href="https://en.wikipedia.org/wiki/HSL_and_HSV" target="_blank">HSL and HSV on Wikipedia</a></li>
					<li><a href="https://realpython.com/python-opencv-color-spaces/" target="_blank">OpenCV Color Spaces</a></li>
					<li><a href="https://docs.opencv.org/3.4/d0/d86/tutorial_py_image_arithmetics.html" target="_blank">OpenCV Image Arithmetics</a></li>
				</ul>
				  
				<li>Utilizing the Color Mask, I can find which pixel values meet the color criteria, allowing us to track a specific color throughout the image. In the video, we use a "Bitwise-And" Mask to show only pixels meeting HSV criteria, making the rest black. (Video Below)</li>
                <br>
				<label>Color Tracking Video Example</label>
				<br>
				<video width="50%" controls>
                    <source src="Team\progress\img - member3\HSV_Color_Filter.mp4" type="video/mp4">
                </video> 
				<br>
				<li>Researched the Lucas-Kanade Optical Flow method as an alternative gloves tracking algorithm.</li>
				<ul>
					<li><a href="https://www.cs.cmu.edu/~16385/s15/lectures/Lecture21.pdf" target="_blank">Lucas-Kanade Optical Flow</a></li>
					<li><a href="https://docs.opencv.org/3.4/db/d7f/tutorial_js_lucas_kanade.html" target="_blank">OpenCV Lucas-Kanade Tutorial</a></li>
				
				</ul>
				<li>Started researching k-means image segmentation for real-time tracking of two gloves, even if they are the same color.</li>
				<ul>
					<li><a href="https://www.kdnuggets.com/2019/08/introduction-image-segmentation-k-means-clustering.html" target="_blank">Introduction to Image Segmentation with K-means Clustering</a></li>
					<li><a href="https://medium.com/mlearning-ai/k-means-clustering-71a875dbce3c" target="_blank">K-means Clustering Article</a></li>
					<li><a href="https://cierra-andaur.medium.com/using-k-means-clustering-for-image-segmentation-fe86c3b39bf4" target="_blank">Using K-means Clustering for Image Segmentation</a></li>
					<li><a href="https://www.geeksforgeeks.org/image-segmentation-using-k-means-clustering/" target="_blank">Image Segmentation using K-means Clustering (GeeksforGeeks)</a></li>
					<li><a href="https://towardsdatascience.com/image-color-segmentation-by-k-means-clustering-algorithm-5792e563f26e" target="_blank">Image Color Segmentation by K-means Clustering</a></li>
				</ul>
			  </ul>
            <b>Date Reported:</b> 01/09/2024<br>
            <b>Start Time:</b> 9:30am<br>
            <b>Work Time:</b>6.5 hours<br>
            <ul>
				<li>I spent this week working on the design specifications of the project in order to find the hardware necessary to complete the project.</li>
			</ul>
			Kinematics & Punch Research:
			<ul>
				<li>I research time duration and kinematics behind a human punch. I used both online resources and slo-mo camera testing to confirm the theoretical time frame values from the online resources.</li>
				<ol>
					<li><a href="https://boxingscience.co.uk/science-behind-punch/" target="_blank">science-behind-punch</a></li>
					<li><a href="https://chesterrep.openrepository.com/bitstream/handle/10034/623170/FINAL%20THESIS%20FOR%20CHESTERREP.pdf?sequence=1&isAllowed=y" target="_blank">Punch Kinematic Doctoral Thesis</a></li>
				</ol>
				<li>From the research we found the average punch time duration was around 100~200ms.</li>
				<ol>
					<li>The video below was filmed using a iPhone 14 Pro using the slow-mo camera feature and it confirms the theorical calculations.</li>
					<!-- <video width="960" height="540" controls> -->
					<br>
					<label>Punch Timeline Test *WATCH at .25 Speed</label>
					<br>
                    <video width="75%" controls>
						<source src="Team\progress\img - member3\Slowmo_punch.mp4" type="video/mp4">
						
					</video> 
				</ol>
			</ul>
			Camera Specification Research:
			<ul>
				<li>With the knowledge of the time requirements, we set out to figure out the minimum frame rate and resolution of the camera.</li>
				<ul>
				  <li>We wanted a minimum resolution of 1 (pixel / cm<sup>2</sup>), to approximately allow our camera system to have around 1 cm<sup>2</sup> accuracy while tracking real-world targets. (Formula Below)</li>
                  <img src="Team\progress\img - member3\CameraRes_Formula.png" width="75%"></img>
				  <li>Since punches have a time frame of 100~200ms, we wanted a camera that would give us around 20 frames to track a punch before impact.</li>
				</ul>
				<li>After calculating our minimum parameters, we asked Joe if he had any cameras that met our specifications. He lent us a <a href="https://www.webcamerausb.com/elp-wide-angle-260fps-usb-webcam-2megapixels-cmos-ov4689-sensor-mini-1080p-60fps-camera-board-module-with-29mm-lens-p-269.html" target="_blank">ELP-USBFHD08S-L29</a> camera.</li>
			  </ul>
            Environment Sensor Placement Structure:
            <ul>
                <li>For our project, we found that a top-down camera view would enable us to accurately track objects in the xy-plane.</li>
        
                <li><strong>Key Advantages</strong></li>
                <ul>
                <li>Utilizes only 1 camera for tracking and means less equipment.</li>
                <li>Simplifies the image processing and lowers computational complexity.</li>
                <li>Allows us to easily map images to real-world coordinate locations.</li>
                </ul>
                <li>We decided on a Tepee Structure to hold the camera and processing unit above the robot environment area. (Diagram Below)</li>
                
            </ul>
            <img src="Team\progress\img - member3\Dodgebot_Structure.jpg" width="75%"></img>
        </div>
		
		
		<!-- Instantiate global footer. Any changes to the footer should be made through the top-level file "footer.html" -->
		<div id="footer"></div>
    </div>
</div>

<!--JS-->
<script src="js/jquery.js"></script>
<script src="js/jquery-migrate-1.1.1.js"></script>

<script type="text/javascript">
$(document).ready(function() {
    $("#header").load("header.html");
	$("#menu").load("navbar.html");
	$("#footer").load("footer.html");
});
</script>
</body>
</html>
